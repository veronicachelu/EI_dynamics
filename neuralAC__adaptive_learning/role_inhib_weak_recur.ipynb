{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import importlib\n",
    "import experiment\n",
    "import plotting\n",
    "\n",
    "# Reload the modules after editing\n",
    "importlib.reload(experiment)\n",
    "importlib.reload(plotting)\n",
    "\n",
    "# Re-import the functions if necessary\n",
    "from experiment import sensitivity_sweep\n",
    "from plotting import plot_results_2cols"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-11T20:32:43.064521Z",
     "start_time": "2024-12-11T20:32:42.971898Z"
    }
   },
   "id": "initial_id",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def default_params():\n",
    "    \"\"\"\n",
    "    Defines the default parameters for the experiment.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing default parameter values.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"time_steps\": 10000,  # Total number of time steps for the experiment\n",
    "        \"task_dist\": \"DetPRL\",  # Task distribution type (e.g., Deterministic PRL)\n",
    "        \"init_reward_means\": ([1, 0], [1, 0]),  # Initial reward means for tasks\n",
    "        \"init_reward_probs\": (0.8, 0.2),  # Initial reward probabilities for tasks\n",
    "        \"task_reward_probs\": None,  # Probabilities for task-specific rewards (if applicable)\n",
    "        \"n_tasks\": 10,  # Number of tasks in the experiment\n",
    "        \"reward_prob\": 1.0,  # Probability of receiving a reward\n",
    "        \"reward_var\": 0.5,  # Variance of the reward noise\n",
    "        \"n_trials\": 20,  # Number of trials for averaging results\n",
    "        \"wI\": [0.0, 0.01, -0.01],  # Exploitative weight values for sensitivity sweep\n",
    "        \"wE\": 0.05,  # Exploratory weight for sensitivity sweep\n",
    "        \"alpha\": 0.01,  # Learning rate\n",
    "        \"tau\": 0.01  # Entropy regularization coefficient\n",
    "    }\n",
    "    return params\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-11T20:33:34.102564Z",
     "start_time": "2024-12-11T20:33:34.091555Z"
    }
   },
   "id": "d16b1e14270de1a2",
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Role of Inhibition in Adaptive Learning:\n",
    " Showing the policy performance for two settings of the\n",
    "policy Q-value precision (scaling, policy learning rate) wE , high (Top) and low (Bottom), and two choices of\n",
    "inhibition strength wI , positive (Left) and negative (Right), averaged over 20 trials\n",
    " Shades denote standard\n",
    "deviation. (Top-Bottom) Uncertainty elicited by continual learning and reward noise is amplified by high\n",
    "policy precision (red baseline)\n",
    "\n",
    " (Bottom) Negative inhibition can amplify policy updates with too low precision\n",
    "(Right), whereas positive inhibition slows down learning"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "99ff16ba93b07047"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load default parameters\n",
    "params = default_params()\n",
    "\n",
    "# Run the sensitivity analysis, sweeping over wI\n",
    "histories, environment = sensitivity_sweep(params, sweep_over=\"wI\")\n",
    "\n",
    "# Plot the results\n",
    "plot_results_2cols(params, histories, environment, \"avg_reward\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-12-11T20:33:45.150274Z"
    }
   },
   "id": "fd4d4efb2dd5c494",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Effect of the Policy Precision wE on Entropy\n",
    "Showing the policy entropy for two settings of the precision wE (Top-Bottom) and two settings of positive and negative inhibition (Left-Right)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b7d05e745260c5f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_results_2cols(params, histories, environment, \"entropy\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "356dc9a8abe719b",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_results_2cols(params, histories, environment, \"V_E\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a99c7eea3f60efcd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_results_2cols(params, histories, environment, \"V_I\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ff8acdd45d9746ae",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-11T20:26:39.384705Z",
     "start_time": "2024-12-11T20:26:39.381092Z"
    }
   },
   "id": "43fba904a9f14c07",
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "psilocybin_and_stress",
   "language": "python",
   "display_name": "psilocybin_and_stress"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
