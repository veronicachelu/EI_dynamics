{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import importlib\n",
    "import experiment\n",
    "import plotting\n",
    "\n",
    "# Reload the modules after editing\n",
    "importlib.reload(experiment)\n",
    "importlib.reload(plotting)\n",
    "\n",
    "# Re-import the functions if necessary\n",
    "from experiment import sensitivity_sweep\n",
    "from plotting import plot_results_2cols"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-11T20:34:04.145374Z",
     "start_time": "2024-12-11T20:34:03.159229Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "small variance\n",
    "high wE\n",
    "inhibition stabilizes, decreases variance\n",
    "task dist - deter prl switch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a155da211dfc78b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def default_params():\n",
    "    \"\"\"\n",
    "    Defines the default parameters for the experiment.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the default parameter values.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"time_steps\": 10000,          # Total number of time steps in the experiment\n",
    "        \"task_dist\": \"DetPRL\",        # Type of task distribution (e.g., Deterministic PRL)\n",
    "        \"init_reward_means\": ([1, 0], [1, 0]),  # Initial reward means for each task\n",
    "        \"init_reward_probs\": (0.8, 0.2),  # Initial probabilities for task rewards\n",
    "        \"task_reward_probs\": None,   # Probabilities for rewards in tasks (optional)\n",
    "        \"n_tasks\": 10,               # Number of tasks in the experiment\n",
    "        \"reward_prob\": 1.0,          # Probability of receiving a reward\n",
    "        \"reward_var\": 0.5,           # Variance of reward noise\n",
    "        \"n_trials\": 20,              # Number of trials to average results\n",
    "        \"wI\": [0.0, 0.01, -0.01],    # List of exploitative weight values for sensitivity sweeps\n",
    "        \"wE\": 0.2,                   # Exploratory weight for policy updates\n",
    "        \"alpha\": 0.01,               # Learning rate for value updates\n",
    "        \"tau\": 0.01                  # Entropy regularization coefficient\n",
    "    }\n",
    "    return params\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-11T20:34:04.168800Z",
     "start_time": "2024-12-11T20:34:04.159419Z"
    }
   },
   "id": "d16b1e14270de1a2",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Role of Inhibition in Adaptive Learning:\n",
    " Showing the policy performance for two settings of the\n",
    "policy Q-value precision (scaling, policy learning rate) wE , high (Top) and low (Bottom), and two choices of\n",
    "inhibition strength wI , positive (Left) and negative (Right), averaged over 20 trials\n",
    " Shades denote standard\n",
    "deviation. (Top-Bottom) Uncertainty elicited by continual learning and reward noise is amplified by high\n",
    "policy precision (red baseline)\n",
    "\n",
    "\n",
    " (Top) This effect is stabilized by positive inhibition (Left) and amplified by\n",
    "negative inhibition (Right)\n",
    " "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "28f731d8f48c59a7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Load default parameters\n",
    "params = default_params()\n",
    "\n",
    "# Run the sensitivity analysis, sweeping over wI\n",
    "histories, environment = sensitivity_sweep(params, sweep_over=\"wI\")\n",
    "\n",
    "# Plot the results\n",
    "plot_results_2cols(params, histories, environment, \"avg_reward\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-12-11T20:34:04.168529Z"
    }
   },
   "id": "fd4d4efb2dd5c494",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Effect of the Policy Precision wE on Entropy\n",
    " Showing the policy entropy for two settings of the precision wE (Top-Bottom) and two settings of positive and negative inhibition (Left-Right)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ac6191ff0daa2d61"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "113e35d308264f05"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_results_2cols(params, histories, environment, \"entropy\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "356dc9a8abe719b",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Opposing Q-value Dynamics lead to Pessimism\n",
    " Showing the expected values VE = Eπ [QE (A)]\n",
    "and VI = Eπ [QI (A)] for the setting in Fig 6.6(Top), averaged over 20 trials\n",
    " Shades denote standard deviation.\n",
    " \n",
    "(Top-Left) Positive inhibition has a similar effect to relaxing the policy precision (red baseline), leading to\n",
    "more stable policy dynamics, which is suitable in settings of high uncertainty\n",
    " \n",
    "(Bottom-Left) This effect\n",
    "emerges through the interaction with the inhibitory bias carried out by the auxiliary variables. (Right)\n",
    "Contrary, the precision is amplified by negative inhibition, which leads to more competition between\n",
    "opposing decisions."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b454d64a84e374d1"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "270da2080d495fc4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_results_2cols(params, histories, environment, \"V_E\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a99c7eea3f60efcd",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_results_2cols(params, histories, environment, \"V_I\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "ff8acdd45d9746ae",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "psilocybin_and_stress",
   "language": "python",
   "display_name": "psilocybin_and_stress"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
